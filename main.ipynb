{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the Data\n",
    "In this step we read the source data, study the variables present in it and have a look at some sample data. \n",
    "This will help us in knowing the different columns present in the data set and study their features. \n",
    "We will use Pandas is library to create the data frame which will be used in the subsequent steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
      "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
      "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
      "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
      "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
      "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
      "\n",
      "         V8        V9  ...       V21       V22       V23       V24       V25  \\\n",
      "0  0.098698  0.363787  ... -0.018307  0.277838 -0.110474  0.066928  0.128539   \n",
      "1  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288 -0.339846  0.167170   \n",
      "2  0.247676 -1.514654  ...  0.247998  0.771679  0.909412 -0.689281 -0.327642   \n",
      "3  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321 -1.175575  0.647376   \n",
      "4 -0.270533  0.817739  ... -0.009431  0.798278 -0.137458  0.141267 -0.206010   \n",
      "\n",
      "        V26       V27       V28  Amount  Class  \n",
      "0 -0.189115  0.133558 -0.021053  149.62      0  \n",
      "1  0.125895 -0.008983  0.014724    2.69      0  \n",
      "2 -0.139097 -0.055353 -0.059752  378.66      0  \n",
      "3 -0.221929  0.062723  0.061458  123.50      0  \n",
      "4  0.502292  0.219422  0.215153   69.99      0  \n",
      "\n",
      "[5 rows x 31 columns] \n",
      "\n",
      "Shape of Complete Data Set\n",
      "(284807, 31) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "#Load the creditcard.csv using pandas\n",
    "datainput = pd.read_csv('creditcard.csv')\n",
    "#https://www.kaggle.com/mlg-ulb/creditcardfraud\n",
    "# Print the top 5 records\n",
    "print(datainput[0:5],\"\\n\")\n",
    "# Print the complete shape of the dataset\n",
    "print(\"Shape of Complete Data Set\")\n",
    "print(datainput.shape,\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking the Imbalance in the Data\n",
    "Now we check how the data is distributed among fraudulent and genuine transactions. \n",
    "This gives us an idea of of what percentage of data is expected to be fraudulent. \n",
    "In ml algorithm this is is referred as data imbalance. \n",
    "If most of the transactions is not fraudulent then it becomes difficult to judge few transactions as genuine or not. \n",
    "We use the class column to count the number of fraudulent engine in transactions and \n",
    "then figure out the actual percentage of fraudulent transactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0017304750013189597\n",
      "False Detection Cases: 492\n",
      "True Detection Cases: 284315 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "#Load the creditcard.csv using pandas\n",
    "datainput = pd.read_csv('creditcard.csv')\n",
    "false = datainput[datainput['Class'] == 1]\n",
    "true = datainput[datainput['Class'] == 0]\n",
    "n = len(false)/float(len(true))\n",
    "print(n)\n",
    "print('False Detection Cases: {}'.format(len(datainput[datainput['Class'] == 1])))\n",
    "print('True Detection Cases: {}'.format(len(datainput[datainput['Class'] == 0])),\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Details of Transaction Types\n",
    "We investigate further into the nature of the transactions for each category of fraudulent and non-fraudulent transactions. \n",
    "We try to statistically estimate various parameters like mean standard deviation maximum value minimum value and \n",
    "different percentiles. This is achieved by using the described method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False Detection Cases\n",
      "----------------------\n",
      "count     492.000000\n",
      "mean      122.211321\n",
      "std       256.683288\n",
      "min         0.000000\n",
      "25%         1.000000\n",
      "50%         9.250000\n",
      "75%       105.890000\n",
      "max      2125.870000\n",
      "Name: Amount, dtype: float64 \n",
      "\n",
      "True Detection Cases\n",
      "----------------------\n",
      "count    284315.000000\n",
      "mean         88.291022\n",
      "std         250.105092\n",
      "min           0.000000\n",
      "25%           5.650000\n",
      "50%          22.000000\n",
      "75%          77.050000\n",
      "max       25691.160000\n",
      "Name: Amount, dtype: float64 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "#Load the creditcard.csv using pandas\n",
    "datainput = pd.read_csv('creditcard.csv')\n",
    "\n",
    "#Check for imbalance in data\n",
    "false = datainput[datainput['Class'] == 1]\n",
    "true = datainput[datainput['Class'] == 0]\n",
    "\n",
    "#False Detection Cases\n",
    "print(\"False Detection Cases\")\n",
    "print(\"----------------------\")\n",
    "print(false.Amount.describe(),\"\\n\")\n",
    "\n",
    "#True Detection Cases\n",
    "print(\"True Detection Cases\")\n",
    "print(\"----------------------\")\n",
    "print(true.Amount.describe(),\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Separating features and Label\n",
    "Before we implement the ML algorithm, we need to decide on the features and labels. Which basically means the categorizing the dependent variables and the independent ones. In our dataset the class column is dependent on the rest of all other columns. So we create a data frames for the last column as well as another dataframe for rest of all other columns. These dataframes will be used to train the model that we are going to create."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(284807, 30)\n",
      "(284807,)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "#Load the creditcard.csv using pandas\n",
    "datainput = pd.read_csv('creditcard.csv')\n",
    "#separating features(X) and label(y)\n",
    "# Select all columns except the last for all rows\n",
    "X = datainput.iloc[:, :-1].values\n",
    "# Select the last column of all rows\n",
    "Y = datainput.iloc[:, -1].values\n",
    "\n",
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Model\n",
    "Now we split the data set into two parts. One is for training and another is for testing. The test_size parameter is used to decide what percentage of the data set will be used only for testing. This exercise will help us gain the confidence on the model we are creating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Load the creditcard.csv using pandas\n",
    "datainput = pd.read_csv('creditcard.csv')\n",
    "\n",
    "#separating features(X) and label(y)\n",
    "X = datainput.iloc[:, :-1].values\n",
    "\n",
    "# Select the last column of all rows\n",
    "Y = datainput.iloc[:, -1].values\n",
    "\n",
    "#train_test_split method\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying Decision Tree Classification\n",
    "There are many different kinds of algorithms available to be applied to this situation. But we choose decision tree as our algorithm for classification. Which is a max tree depth of 4 and supply the test sample to predict the values. Finally, we calculate the accuracy of the result from the test to decide on whether to continue further with this algorithm or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "predicted values :\n",
      " [0 0 0 ... 0 0 0]\n",
      "\n",
      "The accuracy score using the DecisionTreeClassifier :  99.9420666409185\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Load the creditcard.csv using pandas\n",
    "#datainput = pd.read_csv('creditcard.csv')\n",
    "\n",
    "#separating features(X) and label(y)\n",
    "X = datainput.iloc[:, :-1].values\n",
    "Y = datainput.iloc[:, -1].values\n",
    "\n",
    "#train_test_split method\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2)\n",
    "\n",
    "#DecisionTreeClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "classifier=DecisionTreeClassifier(max_depth=4)\n",
    "classifier.fit(X_train,Y_train)\n",
    "predicted=classifier.predict(X_test)\n",
    "print(\"\\npredicted values :\\n\",predicted)\n",
    "\n",
    "#Accuracy\n",
    "DT = metrics.accuracy_score(Y_test, predicted) * 100\n",
    "print(\"\\nThe accuracy score using the DecisionTreeClassifier : \",DT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding Evaluation Parameters\n",
    "Once the accuracy level in the above step is acceptable we go on a further evaluation of the model by finding out different parameters. Which use Precision, recall value and F score as our parameters. precision is the fraction of relevant instances among the retrieved instances, while recall is the fraction of the total amount of relevant instances that were actually retrieved. F score provides a single score that balances both the concerns of precision and recall in one number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "predicted values :\n",
      " [0 0 0 ... 0 0 0]\n",
      "\n",
      "The accuracy score using the DecisionTreeClassifier :  99.91748885221726\n",
      "precision\n",
      "0.8144329896907216\n",
      "recall\n",
      "0.7314814814814815\n",
      "f-Score\n",
      "0.7707317073170732\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "#Load the creditcard.csv using pandas\n",
    "#datainput = pd.read_csv('creditcard.csv')\n",
    "#separating features(X) and label(y)\n",
    "\n",
    "X = datainput.iloc[:, :-1].values\n",
    "Y = datainput.iloc[:, -1].values\n",
    "\n",
    "#train_test_split method\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2)\n",
    "\n",
    "#DecisionTreeClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "classifier=DecisionTreeClassifier(max_depth=4)\n",
    "classifier.fit(X_train,Y_train)\n",
    "predicted=classifier.predict(X_test)\n",
    "print(\"\\npredicted values :\\n\",predicted)\n",
    "#\n",
    "# #Accuracy\n",
    "DT = metrics.accuracy_score(Y_test, predicted) * 100\n",
    "print(\"\\nThe accuracy score using the DecisionTreeClassifier : \",DT)\n",
    "#\n",
    "# #Precision\n",
    "print('precision')\n",
    "# Precision = TP / (TP + FP) (Where TP = True Positive, TN = True Negative, FP = False Positive, FN = False Negative).\n",
    "precision = precision_score(Y_test, predicted, pos_label=1)\n",
    "print(precision_score(Y_test, predicted, pos_label=1))\n",
    "\n",
    "#Recall\n",
    "print('recall')\n",
    "# Recall = TP / (TP + FN)\n",
    "recall = recall_score(Y_test, predicted, pos_label=1)\n",
    "print(recall_score(Y_test, predicted, pos_label=1))\n",
    "\n",
    "#f1-score\n",
    "print('f-Score')\n",
    "# F - scores are a statistical method for determining accuracy accounting for both precision and recall.\n",
    "fscore = f1_score(Y_test, predicted, pos_label=1)\n",
    "print(f1_score(Y_test, predicted, pos_label=1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
